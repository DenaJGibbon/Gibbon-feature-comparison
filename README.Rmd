---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Overview
This is the repository for: 'Mel-frequency cepstral coefficients outperform embeddings from pre-trained convolutional neural networks under noisy conditions for discrimination tasks of individual gibbons' (Lakdari et al, under review). The goal of the paper is to compare different approaches of feature extraction for individual discrimination of gibbon female calls. 

Feature extraction was done in both R and Python, and analyses for publication were done in R.


# Data availability
Acoustic data can be downloaded at: 10.5281/zenodo.8205685. 

## Metadata for all data included in the repository.

```{r, echo=FALSE, results='asis'}
library(knitr)
library(kableExtra)

# Create a data frame for metadata
metadata_df1 <- data.frame(
  FileLocation = "data/features" ,
  Description = "A folder containing the different feature sets for each .wav file, along with recorder ID that include location, time, and date. There is also a column for individual ID",
  Date = date(),
  DateType = "Folder",
  Summary = "The folder contains .csv files for acoustic indices, BirdNET, MFCCs, VGGIsh, and Wav2Vec2. For VGGIsh and BirdNET the .csv files are divided by recorder location."  
)

# Format the data frame as a kable table

# Metadata table 2 --------------------------------------------------------
# Create a data frame for metadata
metadata_df2 <- data.frame(
  FileLocation = "data/MB Playbacks 50 m.csv"  ,
  Description = "A .csv file containing the GPS coordinates of the recorders",
  Date = date(),
  DateType = ".csv",
  Summary = "This file contains the GPS coordinates of each recorder M01-M09"  
)

# Metadata table 2 --------------------------------------------------------
# Create a data frame for metadata
metadata_df3 <- data.frame(
  FileLocation = "data/randomization_affinity"  ,
  Description = "Contains a .csv files for each feature type",
  Date = date(),
  DateType = "Folder",
  Summary = "This file contains the classification accuracy, recorder, number of clusters returned by affinity propagation clustering, and normalized mutual information value "  
)


# Create a data frame for metadata
metadata_df4 <- data.frame(
  FileLocation = "data/randomization_hdbscan"  ,
  Description = "Contains a .csv files for each feature type",
  Date = date(),
  DateType = "Folder",
  Summary = "This file contains the classification accuracy, recorder, number of clusters returned by hdbscan, and normalized mutual information value "  
)

# Create a data frame for metadata
metadata_df5 <- data.frame(
  FileLocation = "data/snr_df"  ,
  Description = "Contains a .csv files for each recorder location",
  Date = date(),
  DateType = "Folder",
  Summary = "This file contains the recording ID, signal-to-noise ratio, recorder, and wave file path."  
)


print(kable(rbind.data.frame(metadata_df1, metadata_df2,metadata_df3,metadata_df4,metadata_df5)))
# Format the data frame as a kable table

```

# Feature/embedding extraction on the audio data

### MFCCs
MFCCs are calculated using the 'Processing features for randomization.R' R script. 

### BirdNET
Follow the installation instructions here: https://github.com/kahst/BirdNET-Analyzer. Then use the 'BirdNET Terminal Script'. Then run the 'Processing features for randomization.R' R script to convert BirdNET embeddings into the format needed for analyses.

### VGGish
Follow installation instructions: https://github.com/tensorflow/models/blob/master/research/audioset/vggish/README.md. Then use the 'VGGish Terminal Script'. Then run the 'Processing features for randomization.R' R script to convert VGGish embeddings into the format needed for analyses.

### Wav2Vec2
Wav2Vec2 embeddings are caluclated using the 'Wav2Vec2_Features.py' Python script.

### Acoustic indices
Acoustic indices are calculated using the 'Processing features for randomization.R' R script. 

# SNR calculation on audio data
SNR calculation is done on sound clips that have an extra 2-s on either side of the call using the 'SNR Calculation' R script.

# Supervised classification and unsupervised clustering of processed data
Use the 'Randomization for playbacks.R' script to randomly divide data for each feature and distance category using a 80/20 split. This script uses the processed data for each feature located in the 'data/features/features' folder.

# Creating plots for publication
See 'Script to recreate figures.R' to recreate all figures in publication.

```{r,message=FALSE, echo=FALSE, fig.cap = "Figure 1. Uniform Manifold Approximation and Projections (UMAP) of female gibbon calls recorded ~ 50 m away from the playback speaker for each feature type."}

library(ggpubr)
library(umap)
library(stringr)
library(cowplot)
library(viridis)

# MFCC UMAP Plots ------------------------------------------------------

# Read MFCCPlaybacks.csv
MFCCPlaybacks <- read.csv('data/features/MFCCPlaybacks.csv')

# Extract the recorder information from the 'Recording' column
MFCCPlaybacks$Recorder <- str_split_fixed(MFCCPlaybacks$Recording, pattern = '_', n = 2)[, 1]

# Get unique recorders
UniqueRecorderMFCC <- unique(MFCCPlaybacks$Recorder)

# Convert 'Individual' column to a factor
MFCCPlaybacks$Individual <- as.factor(MFCCPlaybacks$Individual)

# Subset the data for the current recorder
MFCCPlaybacksSingleRecorderM2 <- droplevels(subset(MFCCPlaybacks, Recorder == 'M2'))
MFCCPlaybacksSingleRecorderM6 <- droplevels(subset(MFCCPlaybacks, Recorder == 'M6'))

MFCCM2.umap <-
  umap::umap(MFCCPlaybacksSingleRecorderM2 [, -c(26:28)],
             #labels=as.factor(MFCC$Validation),
             controlscale=TRUE,scale=3,n_neighbors=5)

plot.for.MFCCM2 <-
  cbind.data.frame(MFCCM2.umap$layout[,1:2],MFCCPlaybacksSingleRecorderM2 $Individual)

colnames(plot.for.MFCCM2) <-
  c("Dim.1", "Dim.2","Class")


MFCCM2Scatter <- ggpubr::ggscatter(data = plot.for.MFCCM2,x = "Dim.1",
                                    y = "Dim.2",
                                    color='Class')+guides(color='none')+
  scale_color_manual(values =viridis::viridis (length(
    unique(plot.for.MFCCM2$Class)
  ))) + guides(color="none")+ggtitle( paste('MFCCs'))+theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),axis.text.y=element_blank(),
    axis.ticks.y=element_blank())+   theme(plot.title = element_text(hjust = 1))  +
  theme(plot.title = element_text(hjust = 1))  


# BirdNET UMAP Plots ---------------------------------------------------
file_names <- dir('data/features/BirdNET/', full.names = T)#where you have your files

BirdNETPlaybacks <- do.call(rbind,lapply(file_names,read.csv))

# Extract the recorder information from the 'Recording' column
BirdNETPlaybacks$Recorder <- as.factor(BirdNETPlaybacks$Recorder)

# Get unique recorders
UniqueRecorderBirdNET <- unique(BirdNETPlaybacks$Recorder)

# Convert 'Individual' column to a factor
BirdNETPlaybacks$Individual <- as.factor(BirdNETPlaybacks$Individual)

# Subset the data for the current recorder
BirdNETPlaybacksSingleRecorderM2 <- droplevels(subset(BirdNETPlaybacks, Recorder == 'M2'))
BirdNETPlaybacksSingleRecorderM6 <- droplevels(subset(BirdNETPlaybacks, Recorder == 'M6'))

BirdNETM2.umap <-
  umap::umap(BirdNETPlaybacksSingleRecorderM2 [, -c(2048:2050,2052)],
             #labels=as.factor(BirdNET$Validation),
             controlscale=TRUE,scale=3,n_neighbors=5)

plot.for.BirdNETM2 <-
  cbind.data.frame(BirdNETM2.umap$layout[,1:2],BirdNETPlaybacksSingleRecorderM2 $Individual)

colnames(plot.for.BirdNETM2) <-
  c("Dim.1", "Dim.2","Class")


BirdNETM2Scatter <- ggpubr::ggscatter(data = plot.for.BirdNETM2,x = "Dim.1",
                                   y = "Dim.2",
                                   color='Class')+guides(color='none')+
  scale_color_manual(values =viridis::viridis (length(
    unique(plot.for.BirdNETM2$Class)
  ))) + guides(color="none")+ggtitle( paste('BirdNET'))+theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),axis.text.y=element_blank(),
    axis.ticks.y=element_blank())+   theme(plot.title = element_text(hjust = 1))  



# VGGish UMAP Plots ---------------------------------------------------
file_names <- dir('data/features/VGGish/', full.names = T)#where you have your files

VGGishPlaybacks <- do.call(rbind,lapply(file_names,read.csv))

# Extract the recorder information from the 'RecorderID' column
VGGishPlaybacks$Recorder <- str_split_fixed(VGGishPlaybacks$RecorderID, pattern = '_', n = 2)[, 1]

VGGishPlaybacks$Recorder <- as.factor(VGGishPlaybacks$Recorder)

# Get unique recorders
UniqueRecorderVGGish <- unique(VGGishPlaybacks$Recorder)

# Convert 'Individual' column to a factor
VGGishPlaybacks$Individual <- as.factor(VGGishPlaybacks$Individual)

# Subset the data for the current recorder
VGGishPlaybacksSingleRecorderM2 <- droplevels(subset(VGGishPlaybacks, Recorder == 'M2'))
VGGishPlaybacksSingleRecorderM6 <- droplevels(subset(VGGishPlaybacks, Recorder == 'M6'))

VGGishM2.umap <-
  umap::umap(VGGishPlaybacksSingleRecorderM2 [, -c(257,259,260)],
             #labels=as.factor(VGGish$Validation),
             controlscale=TRUE,scale=3,n_neighbors=5)

plot.for.VGGishM2 <-
  cbind.data.frame(VGGishM2.umap$layout[,1:2],VGGishPlaybacksSingleRecorderM2 $Individual)

colnames(plot.for.VGGishM2) <-
  c("Dim.1", "Dim.2","Class")


VGGishM2Scatter <- ggpubr::ggscatter(data = plot.for.VGGishM2,x = "Dim.1",
                                      y = "Dim.2",
                                      color='Class')+guides(color='none')+
  scale_color_manual(values =viridis::viridis (length(
    unique(plot.for.VGGishM2$Class)
  ))) + guides(color="none")+ggtitle( paste('VGGish'))+theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),axis.text.y=element_blank(),
    axis.ticks.y=element_blank())+   theme(plot.title = element_text(hjust = 1))  



# wav2vec2 UMAP Plots ---------------------------------------------------
# Read wav2vec2Playbacks.csv
wav2vec2Playbacks <- read.csv('data/features/wav2vecmeansdDFPlayback.csv')

# Extract the recorder information from the 'Recording' column
wav2vec2Playbacks$Recorder <- str_split_fixed(wav2vec2Playbacks$RecorderID, pattern = '_', n = 2)[, 1]

# Get unique recorders
UniqueRecorderwav2vec2 <- unique(wav2vec2Playbacks$Recorder)

# Convert 'Individual' column to a factor
wav2vec2Playbacks$Individual <- as.factor(wav2vec2Playbacks$Individual)

# Get unique recorders
UniqueRecorderwav2vec2 <- unique(wav2vec2Playbacks$Recorder)

# Convert 'Individual' column to a factor
wav2vec2Playbacks$Individual <- as.factor(wav2vec2Playbacks$Individual)

# Subset the data for the current recorder
wav2vec2PlaybacksSingleRecorderM2 <- droplevels(subset(wav2vec2Playbacks, Recorder == 'M2'))
wav2vec2PlaybacksSingleRecorderM6 <- droplevels(subset(wav2vec2Playbacks, Recorder == 'M6'))

wav2vec2M2.umap <-
  umap::umap(wav2vec2PlaybacksSingleRecorderM2 [, -c(24:44)],
             #labels=as.factor(wav2vec2$Validation),
             controlscale=TRUE,scale=3,n_neighbors=5)

plot.for.wav2vec2M2 <-
  cbind.data.frame(wav2vec2M2.umap$layout[,1:2],wav2vec2PlaybacksSingleRecorderM2 $Individual)

colnames(plot.for.wav2vec2M2) <-
  c("Dim.1", "Dim.2","Class")


wav2vec2M2Scatter <- ggpubr::ggscatter(data = plot.for.wav2vec2M2,x = "Dim.1",
                                     y = "Dim.2",
                                     color='Class')+guides(color='none')+
  scale_color_manual(values =viridis::viridis (length(
    unique(plot.for.wav2vec2M2$Class)
  ))) + guides(color="none")+ggtitle( paste('Wav2Vec2'))+theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),axis.text.y=element_blank(),
    axis.ticks.y=element_blank())+   theme(plot.title = element_text(hjust = 1))  




# AcousticIndices UMAP Plots ---------------------------------------------------
AcousticIndicesPlaybacks <- read.csv('data/features/AcousticIndicesDFPlayback.csv')

# Extract the recorder information from the 'Recording' column
AcousticIndicesPlaybacks$Recorder <- str_split_fixed(AcousticIndicesPlaybacks$RecorderID, pattern = '_', n = 2)[, 1]

# Get unique recorders
UniqueRecorderAcousticIndices <- unique(AcousticIndicesPlaybacks$Recorder)

# Convert 'Individual' column to a factor
AcousticIndicesPlaybacks$Individual <- as.factor(AcousticIndicesPlaybacks$Individual)

# Convert 'Individual' column to a factor
AcousticIndicesPlaybacks$Individual <- as.factor(AcousticIndicesPlaybacks$Individual)

# Subset the data for the current recorder
AcousticIndicesPlaybacksSingleRecorderM2 <- droplevels(subset(AcousticIndicesPlaybacks, Recorder == 'M2'))
AcousticIndicesPlaybacksSingleRecorderM6 <- droplevels(subset(AcousticIndicesPlaybacks, Recorder == 'M6'))

AcousticIndicesM2.umap <-
  umap::umap(AcousticIndicesPlaybacksSingleRecorderM2 [, -c(6:8)],
             #labels=as.factor(AcousticIndices$Validation),
             controlscale=TRUE,scale=3,n_neighbors=5)

plot.for.AcousticIndicesM2 <-
  cbind.data.frame(AcousticIndicesM2.umap$layout[,1:2],AcousticIndicesPlaybacksSingleRecorderM2 $Individual)

colnames(plot.for.AcousticIndicesM2) <-
  c("Dim.1", "Dim.2","Class")


AcousticIndicesM2Scatter <- ggpubr::ggscatter(data = plot.for.AcousticIndicesM2,x = "Dim.1",
                                       y = "Dim.2",
                                       color='Class')+guides(color='none')+
  scale_color_manual(values =viridis::viridis (length(
    unique(plot.for.AcousticIndicesM2$Class)
  ))) + guides(color="none")+ggtitle( paste('AcousticIndices'))+theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),axis.text.y=element_blank(),
    axis.ticks.y=element_blank())+   theme(plot.title = element_text(hjust = 1))  


cowplot::plot_grid(AcousticIndicesM2Scatter,
                   BirdNETM2Scatter,
                   MFCCM2Scatter,
                   VGGishM2Scatter,wav2vec2M2Scatter
                   )



```



